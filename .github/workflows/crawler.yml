name: Katana Web Crawler

on:
  schedule:
    # æ¯å¤© UTC æ—¶é—´ 02:00 è¿è¡Œï¼ˆåŒ—äº¬æ—¶é—´ 10:00ï¼‰
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      target_urls:
        description: 'ç›®æ ‡URLåˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰'
        required: false
        default: 'https://example.com'
      headless_mode:
        description: 'å¯ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼'
        required: false
        default: 'false'
        type: choice
        options:
          - 'true'
          - 'false'
      crawl_depth:
        description: 'çˆ¬å–æ·±åº¦'
        required: false
        default: '3'
        type: number
      crawl_duration:
        description: 'çˆ¬å–æŒç»­æ—¶é—´ï¼ˆå¦‚: 30m, 1hï¼‰'
        required: false
        default: '30m'
      katana_version:
        description: 'Katana ç‰ˆæœ¬'
        required: false
        default: 'v1.2.1'

env:
  # é»˜è®¤é…ç½®
  DEFAULT_URLS: "https://example.com,https://httpbin.org"
  CRAWL_DEPTH: ${{ github.event.inputs.crawl_depth || '3' }}
  CRAWL_DURATION: ${{ github.event.inputs.crawl_duration || '30m' }}
  HEADLESS_MODE: ${{ github.event.inputs.headless_mode || 'false' }}
  TARGET_URLS: ${{ github.event.inputs.target_urls || 'https://example.com,https://httpbin.org' }}
  KATANA_VERSION: ${{ github.event.inputs.katana_version || 'v1.2.1' }}

jobs:
  crawl:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: ğŸŒ è®¾ç½® Chrome æµè§ˆå™¨
      if: env.HEADLESS_MODE == 'true'
      run: |
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        google-chrome --version

    - name: ğŸ“¦ å®‰è£… Katana
      run: |
        KATANA_URL="https://github.com/projectdiscovery/katana/releases/download/${KATANA_VERSION}/katana_${KATANA_VERSION#v}_linux_amd64.zip"
        wget -q "$KATANA_URL" -O katana.zip
        unzip -q katana.zip
        chmod +x katana
        sudo mv katana /usr/local/bin/
        katana -version

    - name: ğŸ“ åˆ›å»ºç›®å½•ç»“æ„
      run: |
        TIMESTAMP=$(date '+%Y/%m/%d')
        mkdir -p "results/$TIMESTAMP"
        mkdir -p "results/final"
        mkdir -p "results/logs"
        mkdir -p "results/stats"
        echo "CRAWL_DATE=$TIMESTAMP" >> $GITHUB_ENV
        echo "CRAWL_TIME=$(date '+%H-%M-%S')" >> $GITHUB_ENV

    - name: ğŸ¯ å‡†å¤‡ç›®æ ‡ URL
      run: |
        echo "$TARGET_URLS" | tr ',' '\n' > results/target_urls.txt
        cat results/target_urls.txt

    - name: ğŸ•·ï¸ å¼€å§‹çˆ¬å–
      run: |
        OUTPUT_DIR="results/$CRAWL_DATE"
        OUTPUT_FILE="$OUTPUT_DIR/crawl_${CRAWL_TIME}.jsonl"
        LOG_FILE="results/logs/crawl_${CRAWL_DATE/\//-}_${CRAWL_TIME}.log"
        
        echo "å¼€å§‹æ—¶é—´: $(date)"
        
        # æ„å»ºå¹¶æ‰§è¡Œ Katana å‘½ä»¤
        if [ "$HEADLESS_MODE" = "true" ]; then
          echo "ğŸ–¥ï¸ å¯ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼..."
          katana \
            -list results/target_urls.txt \
            -d $CRAWL_DEPTH \
            -ct $CRAWL_DURATION \
            -c 20 \
            -p 10 \
            -rl 100 \
            -timeout 20 \
            -retry 2 \
            -jc \
            -fx \
            -td \
            -aff \
            -kf all \
            -jsonl \
            -o "$OUTPUT_FILE" \
            -elog "$LOG_FILE" \
            -v \
            -hl \
            -sc \
            -nos \
            -ef png,jpg,jpeg,gif,js,css,ico,woff,woff2,ttf,svg || {
            echo "âš ï¸ Katana æ‰§è¡Œå®Œæˆï¼Œå¯èƒ½å­˜åœ¨éƒ¨åˆ†é”™è¯¯"
            echo "æ£€æŸ¥æ—¥å¿—æ–‡ä»¶: $LOG_FILE"
          }
        else
          katana \
            -list results/target_urls.txt \
            -d $CRAWL_DEPTH \
            -ct $CRAWL_DURATION \
            -c 20 \
            -p 10 \
            -rl 100 \
            -timeout 15 \
            -retry 2 \
            -jc \
            -fx \
            -td \
            -aff \
            -kf all \
            -jsonl \
            -o "$OUTPUT_FILE" \
            -elog "$LOG_FILE" \
            -v \
            -ef png,jpg,jpeg,gif,js,css,ico,woff,woff2,ttf,svg || {
            echo "âš ï¸ Katana æ‰§è¡Œå®Œæˆï¼Œå¯èƒ½å­˜åœ¨éƒ¨åˆ†é”™è¯¯"
            echo "æ£€æŸ¥æ—¥å¿—æ–‡ä»¶: $LOG_FILE"
          }
        fi
        
        echo "ç»“æŸæ—¶é—´: $(date)"
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        if [ -f "$OUTPUT_FILE" ]; then
          echo "âœ… çˆ¬å–å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: $OUTPUT_FILE"
          echo "æ–‡ä»¶å¤§å°: $(du -h "$OUTPUT_FILE" | cut -f1)"
          echo "URLæ•°é‡: $(wc -l < "$OUTPUT_FILE")"
        else
          echo "âŒ æœªç”Ÿæˆè¾“å‡ºæ–‡ä»¶"
          touch "$OUTPUT_FILE"
        fi

    - name: ğŸ”„ å¤„ç†å’Œå»é‡ç»“æœ
      run: |
        OUTPUT_DIR="results/$CRAWL_DATE"
        OUTPUT_FILE="$OUTPUT_DIR/crawl_${CRAWL_TIME}.jsonl"
        FINAL_FILE="results/final/all_urls.txt"
        FINAL_JSONL="results/final/all_results.jsonl"
        STATS_FILE="results/stats/stats_${CRAWL_DATE/\//-}_${CRAWL_TIME}.txt"
        
        # åˆ›å»ºç»Ÿè®¡æ–‡ä»¶
        echo "=== Katana çˆ¬å–ç»Ÿè®¡æŠ¥å‘Š ===" > "$STATS_FILE"
        echo "çˆ¬å–æ—¶é—´: $(date)" >> "$STATS_FILE"
        echo "ç›®æ ‡ URL: $TARGET_URLS" >> "$STATS_FILE"
        echo "çˆ¬å–æ·±åº¦: $CRAWL_DEPTH" >> "$STATS_FILE"
        echo "çˆ¬å–æŒç»­æ—¶é—´: $CRAWL_DURATION" >> "$STATS_FILE"
        echo "æ— å¤´æ¨¡å¼: $HEADLESS_MODE" >> "$STATS_FILE"
        echo "" >> "$STATS_FILE"
        
        # å¤„ç†å½“å‰ç»“æœ
        if [ -f "$OUTPUT_FILE" ] && [ -s "$OUTPUT_FILE" ]; then
          # æå–URLåˆ°çº¯æ–‡æœ¬æ–‡ä»¶
          CURRENT_URLS="$OUTPUT_DIR/urls_${CRAWL_TIME}.txt"
          jq -r '.url // empty' "$OUTPUT_FILE" 2>/dev/null | sort -u > "$CURRENT_URLS" || {
            # å¦‚æœjqå¤±è´¥ï¼Œå°è¯•ç›´æ¥æå–URLï¼ˆå‡è®¾æ¯è¡Œæ˜¯URLï¼‰
            grep -E '^https?://' "$OUTPUT_FILE" | sort -u > "$CURRENT_URLS" || {
              echo "âš ï¸ æ— æ³•è§£æJSONLæ–‡ä»¶ï¼Œåˆ›å»ºç©ºæ–‡ä»¶"
              touch "$CURRENT_URLS"
            }
          }
          
          CURRENT_COUNT=$(wc -l < "$CURRENT_URLS")
          echo "æœ¬æ¬¡çˆ¬å–URLæ•°é‡: $CURRENT_COUNT" >> "$STATS_FILE"
          
          # æ›´æ–°æœ€ç»ˆå»é‡æ–‡ä»¶
          if [ -f "$FINAL_FILE" ]; then
            # åˆå¹¶å¹¶å»é‡
            cat "$FINAL_FILE" "$CURRENT_URLS" | sort -u > "$FINAL_FILE.tmp"
            mv "$FINAL_FILE.tmp" "$FINAL_FILE"
          else
            cp "$CURRENT_URLS" "$FINAL_FILE"
          fi
          
          # æ›´æ–°æœ€ç»ˆJSONLæ–‡ä»¶
          if [ -f "$FINAL_JSONL" ]; then
            cat "$FINAL_JSONL" "$OUTPUT_FILE" > "$FINAL_JSONL.tmp"
            mv "$FINAL_JSONL.tmp" "$FINAL_JSONL"
          else
            cp "$OUTPUT_FILE" "$FINAL_JSONL"
          fi
          
        else
          echo "æœ¬æ¬¡çˆ¬å–URLæ•°é‡: 0" >> "$STATS_FILE"
          touch "$OUTPUT_DIR/urls_${CRAWL_TIME}.txt"
        fi
        
        # è®¡ç®—æ€»ç»Ÿè®¡
        if [ -f "$FINAL_FILE" ]; then
          TOTAL_COUNT=$(wc -l < "$FINAL_FILE")
          echo "ç´¯è®¡å»é‡URLæ€»æ•°: $TOTAL_COUNT" >> "$STATS_FILE"
        else
          echo "ç´¯è®¡å»é‡URLæ€»æ•°: 0" >> "$STATS_FILE"
          touch "$FINAL_FILE"
        fi
        
        # ç›®å½•ç»“æ„ç»Ÿè®¡
        echo "" >> "$STATS_FILE"
        echo "=== ç›®å½•ç»“æ„ ===" >> "$STATS_FILE"
        find results -type f -name "*.txt" -o -name "*.jsonl" -o -name "*.log" | while read file; do
          size=$(du -h "$file" | cut -f1)
          lines=$(wc -l < "$file" 2>/dev/null || echo "0")
          echo "$file ($size, $lines lines)" >> "$STATS_FILE"
        done
        
        echo "ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ä¿å­˜åˆ°: $STATS_FILE"
        cat "$STATS_FILE"

    - name: Generate summary report
      run: |
        echo "ğŸ“‹ ç”Ÿæˆæ€»ç»“æŠ¥å‘Š..."
        
        SUMMARY_FILE="results/SUMMARY.md"
        
        cat > "$SUMMARY_FILE" << 'EOF'
        # Katana çˆ¬è™«ç»“æœæ€»ç»“
        ## ğŸ“Š æœ€æ–°ç»Ÿè®¡

        EOF
        
        # æ·»åŠ æœ€æ–°ç»Ÿè®¡
        LATEST_STATS=$(find results/stats -name "*.txt" | sort | tail -1)
        if [ -f "$LATEST_STATS" ]; then
          echo "### æœ€æ–°çˆ¬å–æŠ¥å‘Š" >> "$SUMMARY_FILE"
          echo '```' >> "$SUMMARY_FILE"
          cat "$LATEST_STATS" >> "$SUMMARY_FILE"
          echo '```' >> "$SUMMARY_FILE"
          echo "" >> "$SUMMARY_FILE"
        fi
        
        # æ·»åŠ æ–‡ä»¶åˆ—è¡¨
        echo "## ğŸ“ æ–‡ä»¶ç»“æ„" >> "$SUMMARY_FILE"
        echo "" >> "$SUMMARY_FILE"
        echo "- \`final/all_urls.txt\` - æ‰€æœ‰å»é‡åçš„ URL åˆ—è¡¨" >> "$SUMMARY_FILE"
        echo "- \`final/all_results.jsonl\` - æ‰€æœ‰è¯¦ç»†ç»“æœï¼ˆJSONLæ ¼å¼ï¼‰" >> "$SUMMARY_FILE"
        echo "- \`YYYY/MM/DD/\` - æŒ‰æ—¥æœŸç»„ç»‡çš„çˆ¬å–ç»“æœ" >> "$SUMMARY_FILE"
        echo "- \`logs/\` - çˆ¬å–æ—¥å¿—æ–‡ä»¶" >> "$SUMMARY_FILE"
        echo "- \`stats/\` - ç»Ÿè®¡æŠ¥å‘Šæ–‡ä»¶" >> "$SUMMARY_FILE"
        echo "" >> "$SUMMARY_FILE"
        
        # æ·»åŠ æœ€è¿‘çš„çˆ¬å–å†å²
        echo "## ğŸ“… æœ€è¿‘çˆ¬å–å†å²" >> "$SUMMARY_FILE"
        echo "" >> "$SUMMARY_FILE"
        find results -name "crawl_*.jsonl" | sort -r | head -10 | while read file; do
          size=$(du -h "$file" | cut -f1)
          echo "- \`$file\` ($size)" >> "$SUMMARY_FILE"
        done
        
        echo "ğŸ“„ æ€»ç»“æŠ¥å‘Šç”Ÿæˆå®Œæˆ: $SUMMARY_FILE"

    - name: ğŸ“¤ æäº¤ç»“æœåˆ°ä»“åº“
      run: |
        git config --local user.email "bot@932.moe"
        git config --local user.name "HelloTools-bot"
        git add results/
        if git diff --staged --quiet; then
          echo "ğŸ“­ æ²¡æœ‰æ–°çš„æ›´æ”¹éœ€è¦æäº¤"
        else
          COMMIT_MSG="ğŸ•·ï¸ Katana crawl results - $(date '+%Y-%m-%d %H:%M:%S')"
          if [ -f "results/final/all_urls.txt" ]; then
            TOTAL_URLS=$(wc -l < "results/final/all_urls.txt")
            COMMIT_MSG="$COMMIT_MSG

        ğŸ“Š Total unique URLs: $TOTAL_URLS
        ğŸ” Depth: $CRAWL_DEPTH | Duration: $CRAWL_DURATION
        ğŸ–¥ï¸ Headless mode: $HEADLESS_MODE"
          fi
          git commit -m "$COMMIT_MSG"
          git push
          echo "âœ… ç»“æœå·²æˆåŠŸæäº¤åˆ°ä»“åº“"
        fi

    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: katana-crawl-results-${{ env.CRAWL_DATE }}-${{ env.CRAWL_TIME }}
        path: |
          results/
        retention-days: 30

    - name: ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆ
      run: |
        echo "ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆï¼"
        echo ""
        echo "ğŸ“Š æœ€ç»ˆç»Ÿè®¡:"
        if [ -f "results/final/all_urls.txt" ]; then
          echo "- ç´¯è®¡ URL æ€»æ•°: $(wc -l < results/final/all_urls.txt)"
        fi
        echo "- æœ¬æ¬¡çˆ¬å–æ—¶é—´: $CRAWL_DATE $CRAWL_TIME"
        echo "- ç›®æ ‡URL: $TARGET_URLS"
        echo "- çˆ¬å–æ·±åº¦: $CRAWL_DEPTH"
        echo "- æŒç»­æ—¶é—´: $CRAWL_DURATION"
        echo "- æ— å¤´æ¨¡å¼: $HEADLESS_MODE"
        echo ""
        echo "ğŸ“ ç»“æœæ–‡ä»¶ä½ç½®:"
        echo "- results/final/all_urls.txt (å»é‡URLåˆ—è¡¨)"
        echo "- results/final/all_results.jsonl (è¯¦ç»†ç»“æœ)"
        echo "- results/$CRAWL_DATE/ (æœ¬æ¬¡ç»“æœ)"
        echo ""
        echo "ğŸ“‹ æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š: results/SUMMARY.md"
